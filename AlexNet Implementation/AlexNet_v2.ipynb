{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from random import randrange\n",
    "import pickle\n",
    "import json\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shuffled_data():\n",
    "    \"\"\"\n",
    "    Load dataset from pickle file and split features and labels\n",
    "    returns (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(\"sample_train_img.pkl\", 'rb') as f_name:\n",
    "        train_img = pickle.load(f_name)\n",
    "        train_img /= 255.0\n",
    "\n",
    "    with open(\"sample_train_label.pkl\", 'rb') as f_name:\n",
    "        train_label = pickle.load(f_name)\n",
    "        train_label = tf.keras.utils.to_categorical(train_label.astype('int'))\n",
    "        \n",
    "    \n",
    "    with open(\"sample_test_img.pkl\", 'rb') as f_name:\n",
    "        test_img = pickle.load(f_name)\n",
    "        test_img /= 255.0\n",
    "\n",
    "    with open(\"sample_test_label.pkl\", 'rb') as f_name:\n",
    "        test_label = pickle.load(f_name)\n",
    "        test_label = tf.keras.utils.to_categorical(test_label.astype('int'))\n",
    "\n",
    "    #Shuffling Images!\n",
    "\n",
    "    permute_train = np.random.permutation(len(train_label))\n",
    "    permute_test = np.random.permutation(len(test_label))\n",
    "\n",
    "    train_img = train_img[permute_train]\n",
    "    train_label = train_label[permute_train]\n",
    "\n",
    "    test_img = train_img[permute_train]\n",
    "    test_label = train_label[permute_train]\n",
    "\n",
    "    return(train_img, test_img, train_label, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(image_height = 256, image_width = 256, image_channel=1, class_count = 2):\n",
    "    \n",
    "    model = Sequential()\n",
    "    #Layer 1\n",
    "    model.add(Conv2D(   filters = 96, \n",
    "                        kernel_size=(11,11), \n",
    "                        strides = 4, \n",
    "                        activation='relu', \n",
    "                        kernel_initializer='he_uniform', \n",
    "                        padding = \"same\",\n",
    "                        input_shape = (image_height, image_width, image_channel)))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #Taking the maximum in that subregion using MaxPooling\n",
    "    model.add(MaxPool2D(    pool_size = (3, 3),\n",
    "                            strides = 2))\n",
    "    #he_uniform: Draw samples from uniform distributions within range of limits created with units\n",
    "\n",
    "\n",
    "    #Layer 2\n",
    "    model.add(Conv2D(   filters = 256, \n",
    "                        kernel_size=(5,5), \n",
    "                        strides = 1,\n",
    "                        activation='relu', \n",
    "                        kernel_initializer='he_uniform'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(    pool_size = (3, 3),\n",
    "                            strides = 2))\n",
    "\n",
    "    #Layer 3: \n",
    "    model.add(Conv2D(   filters = 128, \n",
    "                        kernel_size=(3,3), \n",
    "                        strides = 1, \n",
    "                        activation='relu', \n",
    "                        kernel_initializer='he_uniform'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(    pool_size=(3,3),\n",
    "                            strides = 2))\n",
    "    \n",
    "    #Layer 4\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #Layer 5:\n",
    "    model.add(Dense(    units = 2048, \n",
    "                        activation='relu',              \n",
    "                        kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))   \n",
    "\n",
    "    #Layer 6\n",
    "    model.add(Dense(    units = 2048, \n",
    "                        activation='relu', \n",
    "                        kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))   \n",
    "\n",
    "    #Layer 7\n",
    "    model.add(Dense(    units = 2, \n",
    "                        activation='softmax'))\n",
    "\n",
    "    #Compiling:\n",
    "    opt = SGD(lr = 0.001, momentum = 0.9)\n",
    "    model.compile(  optimizer=opt, \n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN_model()\n",
    "train_x, test_x, train_y, test_y = load_shuffled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(180, 256, 256, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "np.shape(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'AlexNet_v2_Trial_3.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 162 samples, validate on 18 samples\n",
      "Epoch 1/10\n",
      "160/162 [============================>.] - ETA: 0s - loss: 4.3935 - accuracy: 0.5000\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.44444, saving model to AlexNet_v3_Trial_3.h5\n",
      "162/162 [==============================] - 14s 84ms/sample - loss: 4.4259 - accuracy: 0.5000 - val_loss: 1.7982 - val_accuracy: 0.4444\n",
      "Epoch 2/10\n",
      "160/162 [============================>.] - ETA: 0s - loss: 3.0101 - accuracy: 0.6562\n",
      "Epoch 00002: val_accuracy did not improve from 0.44444\n",
      "162/162 [==============================] - 9s 53ms/sample - loss: 2.9729 - accuracy: 0.6605 - val_loss: 7.3015 - val_accuracy: 0.4444\n",
      "Epoch 3/10\n",
      "160/162 [============================>.] - ETA: 0s - loss: 3.4451 - accuracy: 0.6875\n",
      "Epoch 00003: val_accuracy did not improve from 0.44444\n",
      "162/162 [==============================] - 10s 65ms/sample - loss: 3.4026 - accuracy: 0.6914 - val_loss: 7.7258 - val_accuracy: 0.4444\n",
      "Epoch 4/10\n",
      "160/162 [============================>.] - ETA: 0s - loss: 3.0229 - accuracy: 0.8250\n",
      "Epoch 00004: val_accuracy did not improve from 0.44444\n",
      "162/162 [==============================] - 10s 65ms/sample - loss: 3.5322 - accuracy: 0.8148 - val_loss: 11.8188 - val_accuracy: 0.4444\n",
      "Epoch 5/10\n",
      "160/162 [============================>.] - ETA: 0s - loss: 12.7220 - accuracy: 0.6187\n",
      "Epoch 00005: val_accuracy improved from 0.44444 to 0.55556, saving model to AlexNet_v3_Trial_3.h5\n",
      "162/162 [==============================] - 14s 89ms/sample - loss: 12.5649 - accuracy: 0.6235 - val_loss: 44.8268 - val_accuracy: 0.5556\n",
      "Epoch 6/10\n",
      "160/162 [============================>.] - ETA: 0s - loss: 13.1067 - accuracy: 0.6687\n",
      "Epoch 00006: val_accuracy did not improve from 0.55556\n",
      "162/162 [==============================] - 9s 54ms/sample - loss: 12.9449 - accuracy: 0.6728 - val_loss: 39.1227 - val_accuracy: 0.4444\n",
      "Epoch 7/10\n",
      "160/162 [============================>.] - ETA: 0s - loss: 9.4898 - accuracy: 0.7063 \n",
      "Epoch 00007: val_accuracy did not improve from 0.55556\n",
      "162/162 [==============================] - 8s 48ms/sample - loss: 9.3727 - accuracy: 0.7099 - val_loss: 36.0063 - val_accuracy: 0.5556\n",
      "Epoch 8/10\n",
      "160/162 [============================>.] - ETA: 0s - loss: 5.6122 - accuracy: 0.7563\n",
      "Epoch 00008: val_accuracy improved from 0.55556 to 0.72222, saving model to AlexNet_v3_Trial_3.h5\n",
      "162/162 [==============================] - 12s 76ms/sample - loss: 5.5429 - accuracy: 0.7593 - val_loss: 0.7953 - val_accuracy: 0.7222\n",
      "Epoch 9/10\n",
      "160/162 [============================>.] - ETA: 0s - loss: 3.8573 - accuracy: 0.8125\n",
      "Epoch 00009: val_accuracy did not improve from 0.72222\n",
      "162/162 [==============================] - 8s 50ms/sample - loss: 3.8999 - accuracy: 0.8086 - val_loss: 14.3785 - val_accuracy: 0.5556\n",
      "Epoch 10/10\n",
      "160/162 [============================>.] - ETA: 0s - loss: 4.3831 - accuracy: 0.8375\n",
      "Epoch 00010: val_accuracy did not improve from 0.72222\n",
      "162/162 [==============================] - 9s 53ms/sample - loss: 4.3290 - accuracy: 0.8395 - val_loss: 22.9731 - val_accuracy: 0.5556\n"
     ]
    }
   ],
   "source": [
    "#Fit Model!\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10)\n",
    "mc = ModelCheckpoint('AlexNet_v2_Trial_3.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)\n",
    "history = cnn_model.fit(train_x, train_y, epochs = 10, batch_size = 32, validation_split = 0.1, verbose = 1, callbacks = [es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test, Accuracy:  0.51666665\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Data\n",
    "model = load_model('AlexNet_v2_Trial_1.h5')\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose = 0)\n",
    "print(\"Test, Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2048 - Incorrect\n",
    "#Trial 1: 68.9% - 52%\n",
    "#Trail 2: 82.2% - 51.6%\n",
    "#Trail 3: 71.67% - 50%\n",
    "\n",
    "# For 4096 - Different Validation and Test Split!\n",
    "# Trail 1: 55.0% - 51.6%\n",
    "# Trail 2: 55.6% - 67.2&\n",
    "# Trail 3: 72.2% - 73%"
   ]
  }
 ]
}