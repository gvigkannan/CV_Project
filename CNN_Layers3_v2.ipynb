{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from random import randrange\n",
    "import pickle\n",
    "import json\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shuffled_data():\n",
    "    \"\"\"\n",
    "    Load dataset from pickle file and split features and labels\n",
    "    returns (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(\"sample_train_img.pkl\", 'rb') as f_name:\n",
    "        train_img = pickle.load(f_name)\n",
    "        train_img /= 255.0\n",
    "\n",
    "    with open(\"sample_train_label.pkl\", 'rb') as f_name:\n",
    "        train_label = pickle.load(f_name)\n",
    "        train_label = tf.keras.utils.to_categorical(train_label.astype('int'))\n",
    "        \n",
    "    \n",
    "    with open(\"sample_test_img.pkl\", 'rb') as f_name:\n",
    "        test_img = pickle.load(f_name)\n",
    "        test_img /= 255.0\n",
    "\n",
    "    with open(\"sample_test_label.pkl\", 'rb') as f_name:\n",
    "        test_label = pickle.load(f_name)\n",
    "        test_label = tf.keras.utils.to_categorical(test_label.astype('int'))\n",
    "\n",
    "    #Shuffling Images!\n",
    "\n",
    "    permute_train = np.random.permutation(len(train_label))\n",
    "    permute_test = np.random.permutation(len(test_label))\n",
    "\n",
    "    train_img = train_img[permute_train]\n",
    "    train_label = train_label[permute_train]\n",
    "\n",
    "    test_img = train_img[permute_train]\n",
    "    test_label = train_label[permute_train]\n",
    "\n",
    "    return(train_img, test_img, train_label, test_label)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(image_height = 256, image_width = 256, image_channel=1, class_count = 2):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(   filters = 64, \n",
    "                        kernel_size=(9,9), \n",
    "                        strides = 2, \n",
    "                        activation='relu', \n",
    "                        kernel_initializer='he_uniform', \n",
    "                        input_shape = (image_height, image_width, image_channel)))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #Taking the maximum in that subregion using MaxPooling\n",
    "    model.add(MaxPool2D(    pool_size = (3, 3),\n",
    "                            strides = 2))\n",
    "    #he_uniform: Draw samples from uniform distributions within range of limits created with units\n",
    "\n",
    "    model.add(Conv2D(   filters = 256, \n",
    "                        kernel_size=(5,5), \n",
    "                        strides = 1, \n",
    "                        activation='relu', \n",
    "                        kernel_initializer='he_uniform'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(    pool_size = (3, 3),\n",
    "                            strides = 2))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(   filters = 128, \n",
    "                        kernel_size=(3,3), \n",
    "                        strides = 1, \n",
    "                        activation='relu', \n",
    "                        kernel_initializer='he_uniform'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(    pool_size = (3, 3),\n",
    "                            strides = 2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(    units = 4096, \n",
    "                        activation='relu',              \n",
    "                        kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.5))   \n",
    "\n",
    "    model.add(Dense(    units = 1024, \n",
    "                        activation='relu', \n",
    "                        kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.5))   \n",
    "\n",
    "    model.add(Dense(    units = 256, \n",
    "                            activation='relu', \n",
    "                            kernel_initializer='he_uniform'))\n",
    "\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(    units = 2, \n",
    "                        activation='softmax'))\n",
    " \n",
    "\n",
    "    #Compiling:\n",
    "    opt = SGD(lr = 0.001, momentum = 0.9)\n",
    "    model.compile(  optimizer=opt, \n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 124, 124, 64)      5248      \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 124, 124, 64)      256       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 61, 61, 64)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 57, 57, 256)       409856    \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 57, 57, 256)       1024      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 28, 28, 256)       0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 26, 26, 128)       295040    \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 26, 26, 128)       512       \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 12, 12, 128)       0         \n_________________________________________________________________\nflatten (Flatten)            (None, 18432)             0         \n_________________________________________________________________\ndense (Dense)                (None, 1024)              18875392  \n_________________________________________________________________\ndropout (Dropout)            (None, 1024)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1024)              1049600   \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 1024)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 256)               262400    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 2)                 514       \n=================================================================\nTotal params: 20,899,842\nTrainable params: 20,898,946\nNon-trainable params: 896\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN_model()\n",
    "train_x, test_x, train_y, test_y = load_shuffled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.56667, saving model to CNN_v2_Trial_2.h5\n",
      "6/6 - 24s - loss: 3.3685 - accuracy: 0.5444 - val_loss: 0.6996 - val_accuracy: 0.5667\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.56667\n",
      "6/6 - 26s - loss: 3.2210 - accuracy: 0.5889 - val_loss: 1.3761 - val_accuracy: 0.5056\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.56667\n",
      "6/6 - 22s - loss: 4.4941 - accuracy: 0.6333 - val_loss: 1.3236 - val_accuracy: 0.5167\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.56667 to 0.71667, saving model to CNN_v2_Trial_2.h5\n",
      "6/6 - 24s - loss: 3.0907 - accuracy: 0.6833 - val_loss: 0.5716 - val_accuracy: 0.7167\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.71667\n",
      "6/6 - 20s - loss: 3.1725 - accuracy: 0.7167 - val_loss: 2.5869 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.71667\n",
      "6/6 - 24s - loss: 3.5473 - accuracy: 0.6667 - val_loss: 0.9731 - val_accuracy: 0.5389\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.71667\n",
      "6/6 - 21s - loss: 1.9073 - accuracy: 0.6944 - val_loss: 0.6131 - val_accuracy: 0.6778\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.71667\n",
      "6/6 - 35s - loss: 1.0417 - accuracy: 0.7611 - val_loss: 0.5903 - val_accuracy: 0.6833\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.71667\n",
      "6/6 - 25s - loss: 1.3376 - accuracy: 0.7778 - val_loss: 0.6154 - val_accuracy: 0.6778\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.71667\n",
      "6/6 - 36s - loss: 1.2057 - accuracy: 0.8056 - val_loss: 0.6663 - val_accuracy: 0.6056\n"
     ]
    }
   ],
   "source": [
    "#Fit Model!\n",
    "#Replace text_x, test_y with validation data collected!\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10)\n",
    "mc = ModelCheckpoint('CNN_v2_Trial_2.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)\n",
    "history = cnn_model.fit(train_x, train_y, epochs = 10, batch_size = 32, validation_data = (test_x, test_y), verbose = 2, callbacks = [es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Data\n",
    "loss, acc = cnn_model.evaluate(test_x, test_y, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.605555534362793\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}