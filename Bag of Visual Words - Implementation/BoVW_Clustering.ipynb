{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual BoW - Clustering using MiniBatchKMeans!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from sys import getsizeof\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "duration = 1500  # milliseconds\n",
    "freq = 600  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = os.getcwd()\n",
    "car_dir = \"\\\\\".join((path_dir, \"Car_Split\", \"train\"))\n",
    "car_splits = os.listdir(car_dir)\n",
    "noise_dir = \"\\\\\".join((path_dir, \"Noise_Split\", \"train\"))\n",
    "noise_splits = os.listdir(noise_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exacting all the different splits of the car dataset - for train!\n",
    "split_path = \"\\\\\".join((car_dir, car_splits[0]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_0 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((car_dir, car_splits[1]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_1 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((car_dir, car_splits[2]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_2 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((car_dir, car_splits[3]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_3 = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exacting all the different splits of the noise dataset - for train!\n",
    "split_path = \"\\\\\".join((noise_dir, noise_splits[0]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_0 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((noise_dir, noise_splits[1]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_1 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((noise_dir, noise_splits[2]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_2 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((noise_dir, noise_splits[3]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_3 = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#car_pickle_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The Integraty of the Dataset is maintained but the display is all over the place. \n",
    "# # Long live pickle! But too much space!\n",
    "# desc_0 = np.asarray(desc_0)\n",
    "# label_0 = np.array(label_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each image descriptor has a shape (m, n, 32). \n",
    "# Where m = number of descriptors\n",
    "# n = number of descriptor vectors for each descriptor\n",
    "# 32 - number for each vector\n",
    "\n",
    "# We need to reshape such that we get (m x n, 32)\n",
    "# np.shape(car_pickle_0[0][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_descriptor(pickle_file):\n",
    "    print(\"Before Extraction: \\t\", np.shape(pickle_file))\n",
    "    desc_arr = []\n",
    "    for l in pickle_file:\n",
    "        if len(l) != 0:\n",
    "            for desc in l:\n",
    "                desc_arr.append(desc)\n",
    "\n",
    "    print(\"After Extraction: \\t\", np.shape(desc_arr))\n",
    "\n",
    "    return(desc_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before Extraction: \t (1638,)\n",
      "After Extraction: \t (778081, 32)\n",
      "Before Extraction: \t (1694,)\n",
      "After Extraction: \t (836822, 32)\n",
      "Before Extraction: \t (1639,)\n",
      "After Extraction: \t (773890, 32)\n",
      "Before Extraction: \t (1694,)\n",
      "After Extraction: \t (833706, 32)\n",
      "Before Extraction: \t (1638,)\n",
      "After Extraction: \t (773428, 32)\n",
      "Before Extraction: \t (1694,)\n",
      "After Extraction: \t (837641, 32)\n",
      "Before Extraction: \t (1639,)\n",
      "After Extraction: \t (771932, 32)\n",
      "Before Extraction: \t (1694,)\n",
      "After Extraction: \t (837941, 32)\n"
     ]
    }
   ],
   "source": [
    "#Using a function to reshape the images based on descriptor length for clustering!\n",
    "car_desc_0 = reshape_descriptor(car_pickle_0)\n",
    "noise_desc_0 = reshape_descriptor(noise_pickle_0)\n",
    "\n",
    "car_desc_1 = reshape_descriptor(car_pickle_1)\n",
    "noise_desc_1 = reshape_descriptor(noise_pickle_1)\n",
    "\n",
    "car_desc_2 = reshape_descriptor(car_pickle_2)\n",
    "noise_desc_2 = reshape_descriptor(noise_pickle_2)\n",
    "\n",
    "car_desc_3 = reshape_descriptor(car_pickle_3)\n",
    "noise_desc_3 = reshape_descriptor(noise_pickle_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(car_0))\n",
    "# i = 0\n",
    "# car_desc_0 = []\n",
    "# shape_num = []\n",
    "# for l in car_pickle_0:\n",
    "#     if len(l) > 0:\n",
    "#         for desc in l:\n",
    "#             car_desc_0.append(desc)\n",
    "        # car_temp = np.ravel(l).astype(float)\n",
    "        # shape_num.append(np.shape(car_temp))\n",
    "        # car0.append(np.asarray(car_temp).astype(float))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(778081, 32)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(car_desc_0)) # Holy smokes! # (778081, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Extract Noise Features\n",
    "# print(np.shape(noise_0))\n",
    "# i = 0\n",
    "# noise_desc_0 = []\n",
    "# shape_num = []\n",
    "# for l in noise_pickle_0:\n",
    "#     if len(l) > 0:\n",
    "#         for desc in l:\n",
    "#             noise_desc_0.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(noise_desc_0)) # (778081, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape_2num = []\n",
    "# car_ravel_0 = []\n",
    "# for l in car0:\n",
    "#     car_temp = np.ravel(l)\n",
    "#     shape_2num.append(np.shape(car_temp))\n",
    "#     car_ravel_0.append(np.asarray(car_temp).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car_reshape = np.ravel(car_ravel_0)\n",
    "# car_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_no = 1\n",
    "cluster_no = 1500\n",
    "iters = 1000\n",
    "bs = 32\n",
    "rs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "# Let's change the number of cluster and random state to see how it varies!\n",
    "kmeans_batch = MiniBatchKMeans( n_clusters = cluster_no,\n",
    "                                max_iter = iters,\n",
    "                                batch_size = bs,\n",
    "                                random_state = rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MiniBatchKMeans(batch_size=32, compute_labels=True, init='k-means++',\n                init_size=None, max_iter=1000, max_no_improvement=10,\n                n_clusters=1500, n_init=3, random_state=0,\n                reassignment_ratio=0.01, tol=0.0, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "print(kmeans_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MiniBatchKMeans Clustering using Partial Fits!\n",
    "# Vary the different arrangement of fitting to see how it works\n",
    "# Change tolerances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  269.7321586608887\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_0)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  34.203097105026245\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_1)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  34.20071625709534\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_2)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  33.330188512802124\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_3)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  39.347944021224976\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_0)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  39.25786352157593\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_1)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  43.33608889579773\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_2)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  39.8409960269928\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_3)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trial_1_KMeans_c1500_b32_rs0.sav\n"
     ]
    }
   ],
   "source": [
    "name = \"_\".join((\"Trial\",str(trial_no),\"KMeans\", \"c\"+ str(cluster_no), \"b\"+ str(bs), \"rs\"+str(rs)+\".sav\"))\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping the trained model!\n",
    "pickle.dump(kmeans_batch, open(name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run it all!\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}