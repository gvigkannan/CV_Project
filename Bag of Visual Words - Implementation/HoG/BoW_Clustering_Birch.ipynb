{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual BoW - Clustering using OBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "from sys import getsizeof\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, Birch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = os.getcwd()\n",
    "car_dir = \"\\\\\".join((path_dir, \"Car_Split\"))\n",
    "car_splits = os.listdir(car_dir)\n",
    "noise_dir = \"\\\\\".join((path_dir, \"Noise_Split\"))\n",
    "noise_splits = os.listdir(noise_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = os.getcwd()\n",
    "data_dir = \"\\\\\".join((path_dir, \"ORB_Dataset\"))\n",
    "data_dir\n",
    "train_dir = \"\\\\\".join((data_dir, \"train\"))\n",
    "#Collecting Descriptors from Train Data!\n",
    "train_files = os.listdir(train_dir)\n",
    "car_dir = \"\\\\\".join((train_dir, train_files[0]))\n",
    "nocar_dir = \"\\\\\".join((train_dir, train_files[1]))\n",
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_dir = os.getcwd()\n",
    "# car_dir = \"\\\\\".join((path_dir, \"Car_Split\"))\n",
    "# car_splits = os.listdir(car_dir)\n",
    "# noise_dir = \"\\\\\".join((path_dir, \"Noise_Split\"))\n",
    "# noise_splits = os.listdir(noise_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i:\\WPI\\Fall 2020\\Computer Vision - RBE 549\\CV Project\\BoW_Visual\\ORB_Dataset\\train\\train_contains_car_ORB.json\n"
     ]
    }
   ],
   "source": [
    "print(car_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16.02661681175232\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "car_descriptors = []\n",
    "with open(car_dir, 'r') as file_name:\n",
    "    car_descriptors = json.load(file_name)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i:\\WPI\\Fall 2020\\Computer Vision - RBE 549\\CV Project\\BoW_Visual\\ORB_Dataset\\train\\train_no_car_ORB.json\n"
     ]
    }
   ],
   "source": [
    "print(nocar_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6554\n"
     ]
    }
   ],
   "source": [
    "print(len(car_descriptors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15.77786135673523\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "noise_descriptors = []\n",
    "with open(nocar_dir, 'r') as file_name:\n",
    "    noise_descriptors = json.load(file_name)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_descriptor(pickle_file):\n",
    "    start_time = time.time()\n",
    "    print(\"Before Extraction: \\t\", np.shape(pickle_file))\n",
    "    desc_arr = []\n",
    "    for l in pickle_file:\n",
    "        if len(l) != 0:\n",
    "            for desc in l:\n",
    "                desc_arr.append(desc)\n",
    "\n",
    "    print(\"After Extraction: \\t\", np.shape(desc_arr))\n",
    "    print(time.time() - start_time)\n",
    "    return(desc_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before Extraction: \t (6554,)\n",
      "After Extraction: \t (3097331, 32)\n",
      "11.188070297241211\n"
     ]
    }
   ],
   "source": [
    "car_desc = reshape_descriptor(car_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before Extraction: \t (6776,)\n",
      "After Extraction: \t (3346110, 32)\n",
      "11.82544493675232\n"
     ]
    }
   ],
   "source": [
    "noise_desc = reshape_descriptor(noise_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_train_len = len(car_descriptors)\n",
    "noise_train_len = len(noise_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_no = 500\n",
    "iters = 1000\n",
    "bs = 32\n",
    "rs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mBirch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbranching_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcompute_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Implements the Birch clustering algorithm.\n",
      "\n",
      "It is a memory-efficient, online-learning algorithm provided as an\n",
      "alternative to :class:`MiniBatchKMeans`. It constructs a tree\n",
      "data structure with the cluster centroids being read off the leaf.\n",
      "These can be either the final cluster centroids or can be provided as input\n",
      "to another clustering algorithm such as :class:`AgglomerativeClustering`.\n",
      "\n",
      "Read more in the :ref:`User Guide <birch>`.\n",
      "\n",
      ".. versionadded:: 0.16\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "threshold : float, default=0.5\n",
      "    The radius of the subcluster obtained by merging a new sample and the\n",
      "    closest subcluster should be lesser than the threshold. Otherwise a new\n",
      "    subcluster is started. Setting this value to be very low promotes\n",
      "    splitting and vice-versa.\n",
      "\n",
      "branching_factor : int, default=50\n",
      "    Maximum number of CF subclusters in each node. If a new samples enters\n",
      "    such that the number of subclusters exceed the branching_factor then\n",
      "    that node is split into two nodes with the subclusters redistributed\n",
      "    in each. The parent subcluster of that node is removed and two new\n",
      "    subclusters are added as parents of the 2 split nodes.\n",
      "\n",
      "n_clusters : int, instance of sklearn.cluster model, default=3\n",
      "    Number of clusters after the final clustering step, which treats the\n",
      "    subclusters from the leaves as new samples.\n",
      "\n",
      "    - `None` : the final clustering step is not performed and the\n",
      "      subclusters are returned as they are.\n",
      "\n",
      "    - :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n",
      "      is fit treating the subclusters as new samples and the initial data\n",
      "      is mapped to the label of the closest subcluster.\n",
      "\n",
      "    - `int` : the model fit is :class:`AgglomerativeClustering` with\n",
      "      `n_clusters` set to be equal to the int.\n",
      "\n",
      "compute_labels : bool, default=True\n",
      "    Whether or not to compute labels for each fit.\n",
      "\n",
      "copy : bool, default=True\n",
      "    Whether or not to make a copy of the given data. If set to False,\n",
      "    the initial data will be overwritten.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "root_ : _CFNode\n",
      "    Root of the CFTree.\n",
      "\n",
      "dummy_leaf_ : _CFNode\n",
      "    Start pointer to all the leaves.\n",
      "\n",
      "subcluster_centers_ : ndarray,\n",
      "    Centroids of all subclusters read directly from the leaves.\n",
      "\n",
      "subcluster_labels_ : ndarray,\n",
      "    Labels assigned to the centroids of the subclusters after\n",
      "    they are clustered globally.\n",
      "\n",
      "labels_ : ndarray, shape (n_samples,)\n",
      "    Array of labels assigned to the input data.\n",
      "    if partial_fit is used instead of fit, they are assigned to the\n",
      "    last batch of data.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "\n",
      "MiniBatchKMeans\n",
      "    Alternative  implementation that does incremental updates\n",
      "    of the centers' positions using mini-batches.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The tree data structure consists of nodes with each node consisting of\n",
      "a number of subclusters. The maximum number of subclusters in a node\n",
      "is determined by the branching factor. Each subcluster maintains a\n",
      "linear sum, squared sum and the number of samples in that subcluster.\n",
      "In addition, each subcluster can also have a node as its child, if the\n",
      "subcluster is not a member of a leaf node.\n",
      "\n",
      "For a new point entering the root, it is merged with the subcluster closest\n",
      "to it and the linear sum, squared sum and the number of samples of that\n",
      "subcluster are updated. This is done recursively till the properties of\n",
      "the leaf node are updated.\n",
      "\n",
      "References\n",
      "----------\n",
      "* Tian Zhang, Raghu Ramakrishnan, Maron Livny\n",
      "  BIRCH: An efficient data clustering method for large databases.\n",
      "  https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n",
      "\n",
      "* Roberto Perdisci\n",
      "  JBirch - Java implementation of BIRCH clustering algorithm\n",
      "  https://code.google.com/archive/p/jbirch\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.cluster import Birch\n",
      ">>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n",
      ">>> brc = Birch(n_clusters=None)\n",
      ">>> brc.fit(X)\n",
      "Birch(n_clusters=None)\n",
      ">>> brc.predict(X)\n",
      "array([0, 0, 0, 1, 1, 1])\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\vigne\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_birch.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     \n"
     ],
     "name": "stdout"
    }
   ],
   "source": [
    "Birch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_descriptors = np.array(noise_descriptors)\n",
    "noise_split_1 = noise_descriptors[:int(noise_train_len * 0.25)]\n",
    "noise_split_2 = noise_descriptors[int(noise_train_len * 0.25):int(noise_train_len * 0.5)]\n",
    "noise_split_3 = noise_descriptors[int(noise_train_len * 0.5) :int(noise_train_len * 0.75)]\n",
    "noise_split_4 = noise_descriptors[int(noise_train_len * 0.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_descriptors = np.array(car_descriptors)\n",
    "car_split_1 = car_descriptors[:int(car_train_len * 0.25)]\n",
    "car_split_2 = car_descriptors[int(car_train_len * 0.25):int(car_train_len * 0.5)]\n",
    "car_split_3 = car_descriptors[int(car_train_len * 0.5) :int(car_train_len * 0.75)]\n",
    "car_split_4 = car_descriptors[int(car_train_len * 0.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"car_train_desc_split_1_v1.pkl\", 'wb') as file_name:\n",
    "#      pickle.dump(car_split_1, file_name)\n",
    "\n",
    "# with open(\"car_train_desc_split_2_v1.pkl\", 'wb') as file_name:\n",
    "#      pickle.dump(car_split_2, file_name)\n",
    "\n",
    "# with open(\"car_train_desc_split_3_v1.pkl\", 'wb') as file_name:\n",
    "#     pickle.dump(car_split_3, file_name)\n",
    "\n",
    "# with open(\"car_train_desc_split_4_v1.pkl\", 'wb') as file_name:\n",
    "#     pickle.dump(car_split_4, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"noise_train_desc_split_1.pkl\", 'wb') as file_name:\n",
    "#      pickle.dump(noise_split_1, file_name)\n",
    "\n",
    "# with open(\"noise_train_desc_split_2.pkl\", 'wb') as file_name:\n",
    "#      pickle.dump(noise_split_2, file_name)\n",
    "\n",
    "# with open(\"noise_train_desc_split_3.pkl\", 'wb') as file_name:\n",
    "#     pickle.dump(noise_split_3, file_name)\n",
    "\n",
    "# with open(\"noise_train_desc_split_4.pkl\", 'wb') as file_name:\n",
    "#     pickle.dump(noise_split_4, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}