{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "#pickle.loads(kmeans_batch, open(\"KMeans_All_Trial_1.sav\", 'wb'))\n",
    "n_cluster = 800\n",
    "with open(\"KMeans_c_800_b_32_rs_5_v1.sav\", 'rb') as f_name:\n",
    "    kmeans_batch = pickle.load(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each image, a histogram needs to be created!\n",
    "# For each descriptor, a cluster number or centroid label is given.\n",
    "# Then for the entire image, a histogram is generated. \n",
    "# What if we separate the images into grids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's import all the feature descriptors!\n",
    "path_dir = os.getcwd()\n",
    "train_car_dir = \"\\\\\".join((path_dir, \"Car_Split\", \"train\"))\n",
    "car_splits = os.listdir(train_car_dir)\n",
    "train_noise_dir = \"\\\\\".join((path_dir, \"Noise_Split\", \"train\"))\n",
    "noise_splits = os.listdir(train_noise_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['car_train_desc_split_1_v1.pkl',\n",
       " 'car_train_desc_split_2_v1.pkl',\n",
       " 'car_train_desc_split_3_v1.pkl',\n",
       " 'car_train_desc_split_4_v1.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "car_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exacting all the different splits of the car dataset - for train!\n",
    "split_path = \"\\\\\".join((train_car_dir, car_splits[0]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_0 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((train_car_dir, car_splits[1]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_1 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((train_car_dir, car_splits[2]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_2 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((train_car_dir, car_splits[3]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_3 = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exacting all the different splits of the noise dataset - for train!\n",
    "split_path = \"\\\\\".join((train_noise_dir, noise_splits[0]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_0 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((train_noise_dir, noise_splits[1]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_1 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((train_noise_dir, noise_splits[2]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_2 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((train_noise_dir, noise_splits[3]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_3 = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_labels(pickle_file, model):\n",
    "    start_time = time.time()\n",
    "    #pickle_file = car_descriptors\n",
    "    \n",
    "    img_cluster = []\n",
    "    for img_desc in pickle_file:\n",
    "        cluster_desc = []\n",
    "        if len(img_desc)> 0:\n",
    "            #for desc in img_desc:\n",
    "            cluster_desc = model.predict(img_desc)\n",
    "            img_cluster.append(cluster_desc)\n",
    "    print(time.time() - start_time)\n",
    "    return(img_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32.0611789226532\n",
      "27.44362711906433\n",
      "\n",
      "\n",
      "25.742599725723267\n",
      "27.35309386253357\n",
      "\n",
      "\n",
      "25.238300561904907\n",
      "27.663174629211426\n",
      "\n",
      "\n",
      "25.26083731651306\n",
      "27.59008526802063\n"
     ]
    }
   ],
   "source": [
    "car_desc_0 = cluster_labels(pickle_file = car_pickle_0, model = kmeans_batch)\n",
    "noise_desc_0 = cluster_labels(noise_pickle_0, model = kmeans_batch)\n",
    "print(\"\\n\")\n",
    "car_desc_1 = cluster_labels(pickle_file = car_pickle_1, model = kmeans_batch)\n",
    "noise_desc_1 = cluster_labels(pickle_file = noise_pickle_1, model = kmeans_batch)\n",
    "print(\"\\n\")\n",
    "car_desc_2 = cluster_labels(pickle_file = car_pickle_2, model = kmeans_batch)\n",
    "noise_desc_2 = cluster_labels(pickle_file = noise_pickle_2, model = kmeans_batch)\n",
    "print(\"\\n\")\n",
    "car_desc_3 = cluster_labels(pickle_file = car_pickle_3, model = kmeans_batch)\n",
    "noise_desc_3 = cluster_labels(pickle_file = noise_pickle_3, model = kmeans_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_clusters = np.concatenate([car_desc_0, car_desc_1, car_desc_2, car_desc_3])\n",
    "noise_clusters = np.concatenate([noise_desc_0, noise_desc_1, noise_desc_2, noise_desc_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(6548,)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(6774,)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(np.shape(car_clusters))\n",
    "display(np.shape(noise_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_histogram(img_clusters, n_clusters = 500):\n",
    "    start_time = time.time()\n",
    "    hist_arr = []\n",
    "    for img in img_clusters:\n",
    "        hist = np.zeros(n_clusters)\n",
    "        for cluster in img:\n",
    "            hist[cluster] += 1\n",
    "        hist_arr.append(hist)\n",
    "    print(time.time() - start_time)\n",
    "    return(hist_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.4896230697631836\n"
     ]
    }
   ],
   "source": [
    "n_cluster = 800\n",
    "#Shouldn't take more than 3 seconds for each function!\n",
    "car_hist = cluster_histogram(img_clusters = car_clusters, n_clusters = n_cluster)\n",
    "noise_hist = cluster_histogram(img_clusters = noise_clusters, n_clusters = n_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.6755881309509277\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df_v1 = pd.DataFrame(car_hist)\n",
    "sscaler = StandardScaler()\n",
    "car_df_ss = pd.DataFrame(sscaler.fit_transform(car_df_v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "print(np.max(car_df_v1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df = pd.DataFrame(car_hist)\n",
    "mm_scalar = MinMaxScaler()\n",
    "car_df_mm = pd.DataFrame(mm_scalar.fit_transform(car_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_df = pd.DataFrame(noise_hist)\n",
    "sscaler = StandardScaler()\n",
    "noise_df_ss = pd.DataFrame(sscaler.fit_transform(noise_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_df = pd.DataFrame(noise_hist)\n",
    "mm_scalar = MinMaxScaler()\n",
    "noise_df_mm = pd.DataFrame(mm_scalar.fit_transform(noise_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   0      1         2    3      4         5         6         7    8     9    \\\n",
       "0  0.0  0.000  0.142857  0.0  0.125  0.083333  0.071429  0.090909  0.0  0.00   \n",
       "1  0.2  0.000  0.357143  0.0  0.250  0.000000  0.071429  0.000000  0.0  0.00   \n",
       "2  0.2  0.000  0.142857  0.0  0.125  0.000000  0.071429  0.000000  0.0  0.25   \n",
       "3  0.2  0.000  0.142857  0.0  0.000  0.000000  0.000000  0.000000  0.0  0.25   \n",
       "4  0.2  0.125  0.142857  0.0  0.250  0.166667  0.000000  0.000000  0.0  0.00   \n",
       "\n",
       "   ...       790    791       792    793  794       795    796       797  \\\n",
       "0  ...  0.000000  0.125  0.000000  0.000  0.0  0.000000  0.000  0.000000   \n",
       "1  ...  0.000000  0.000  0.000000  0.000  0.0  0.016949  0.125  0.000000   \n",
       "2  ...  0.166667  0.000  0.285714  0.000  0.0  0.008475  0.125  0.111111   \n",
       "3  ...  0.000000  0.000  0.142857  0.375  0.0  0.008475  0.000  0.555556   \n",
       "4  ...  0.000000  0.250  0.142857  0.000  0.0  0.008475  0.000  0.000000   \n",
       "\n",
       "        798       799  \n",
       "0  0.111111  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.111111  0.571429  \n",
       "3  0.000000  0.142857  \n",
       "4  0.222222  0.000000  \n",
       "\n",
       "[5 rows x 800 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>790</th>\n      <th>791</th>\n      <th>792</th>\n      <th>793</th>\n      <th>794</th>\n      <th>795</th>\n      <th>796</th>\n      <th>797</th>\n      <th>798</th>\n      <th>799</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.142857</td>\n      <td>0.0</td>\n      <td>0.125</td>\n      <td>0.083333</td>\n      <td>0.071429</td>\n      <td>0.090909</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.125</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n      <td>0.111111</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.2</td>\n      <td>0.000</td>\n      <td>0.357143</td>\n      <td>0.0</td>\n      <td>0.250</td>\n      <td>0.000000</td>\n      <td>0.071429</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.016949</td>\n      <td>0.125</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.2</td>\n      <td>0.000</td>\n      <td>0.142857</td>\n      <td>0.0</td>\n      <td>0.125</td>\n      <td>0.000000</td>\n      <td>0.071429</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>0.166667</td>\n      <td>0.000</td>\n      <td>0.285714</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.008475</td>\n      <td>0.125</td>\n      <td>0.111111</td>\n      <td>0.111111</td>\n      <td>0.571429</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.2</td>\n      <td>0.000</td>\n      <td>0.142857</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>0.142857</td>\n      <td>0.375</td>\n      <td>0.0</td>\n      <td>0.008475</td>\n      <td>0.000</td>\n      <td>0.555556</td>\n      <td>0.000000</td>\n      <td>0.142857</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.2</td>\n      <td>0.125</td>\n      <td>0.142857</td>\n      <td>0.0</td>\n      <td>0.250</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.250</td>\n      <td>0.142857</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.008475</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n      <td>0.222222</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 800 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "car_df_mm.head()"
   ]
  },
  {
   "source": [
    "# noise_df = pd.DataFrame(noise_clusters)\n",
    "# #noise_df = noise_df.fillna(value = -1) \n",
    "# # The problem is that, we cannot assign zero since zero cluster exists. \n",
    "# # We also cannot assign 502 or 501 since we give more priority in that case!\n",
    "# # Only negative value seemed likely \n",
    "# preprocess_sscaler = StandardScaler()\n",
    "# noise_df = preprocess_sscaler.fit_transform(noise_df)\n",
    "# noise_df = pd.DataFrame(noise_clusters)\n",
    "# #noise_df = noise_df.fillna(value = -1) \n",
    "# # The problem is that, we cannot assign zero since zero cluster exists. \n",
    "# # We also cannot assign 502 or 501 since we give more priority in that case!\n",
    "# # Only negative value seemed likely \n",
    "# preprocess_sscaler = MinMaxScaler()\n",
    "# noise_df = pd.DataFrame(preprocess_sscaler.fit_transform(noise_df))\n",
    "# car_df = pd.DataFrame(car_pp_df)\n",
    "# noise_df = pd.DataFrame(noise_df)\n",
    "# sample_car_mm = car_df_mm.drop(columns = [500, 501])\n",
    "# sample_noise_mm = noise_df_mm.drop(columns = [500, 501])\n",
    "\n",
    "# sample_car_ss = car_df_ss.drop(columns = [500, 501])\n",
    "# sample_noise_ss = noise_df_ss.drop(columns = [500, 501])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "sample_car_mm = car_df_mm\n",
    "sample_noise_mm = noise_df_mm\n",
    "\n",
    "sample_car_ss = car_df_ss\n",
    "sample_noise_ss = noise_df_ss"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\nFalse\n"
     ]
    }
   ],
   "source": [
    "# It happens that sometimes there are no freq found for that particular cluster\n",
    "print(np.any(sample_car_mm.isna())) \n",
    "print(np.any(sample_noise_mm.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_car = sample_car.fillna(value = -1)\n",
    "# sample_noise = sample_noise.fillna(value = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\nFalse\n"
     ]
    }
   ],
   "source": [
    "# print(np.any(sample_car_mm.isna())) \n",
    "# print(np.any(sample_noise_mm.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_car_mm.to_csv(\"Car_Hist_MinMax_Max800_v4.csv\", sep = ',', index= False)\n",
    "sample_car_ss.to_csv(\"Car_Hist_SS_Max800_v4.csv\", sep = ',', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_noise_mm.to_csv(\"Noise_Hist_MinMax_Max800_v4.csv\", sep = ',', index= False)\n",
    "sample_noise_ss.to_csv(\"Noise_Hist_SS_Max00_v4.csv\", sep = ',', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Error' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-e5fbc7d54006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#Experiments! Do not run below!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Error' is not defined"
     ]
    }
   ],
   "source": [
    "#Error\n",
    "#Experiments! Do not run below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "399.0    1\n",
       "Name: 501, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "#car_df.iloc[:, -1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  6548.000000  6548.000000  6548.000000  6547.000000  6547.000000   \n",
       "mean      0.496664     0.495801     0.496177     0.493760     0.499219   \n",
       "std       0.285253     0.289804     0.286167     0.287957     0.288914   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.252000     0.246000     0.250000     0.246000     0.248000   \n",
       "50%       0.500000     0.492000     0.490000     0.490000     0.500000   \n",
       "75%       0.740000     0.748000     0.742000     0.744000     0.752000   \n",
       "max       0.998000     0.998000     0.998000     0.998000     0.998000   \n",
       "\n",
       "               5            6            7            8            9    ...  \\\n",
       "count  6547.000000  6546.000000  6545.000000  6545.000000  6544.000000  ...   \n",
       "mean      0.501779     0.489562     0.497996     0.497606     0.493351  ...   \n",
       "std       0.285597     0.285544     0.286654     0.289569     0.288133  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.256000     0.244000     0.254000     0.244000     0.248000  ...   \n",
       "50%       0.500000     0.485000     0.496000     0.500000     0.488000  ...   \n",
       "75%       0.748000     0.736000     0.746000     0.748000     0.742000  ...   \n",
       "max       0.998000     0.998000     0.998000     0.998000     0.998000  ...   \n",
       "\n",
       "               492         493          494          495          496  \\\n",
       "count  4837.000000  4813.00000  4785.000000  4754.000000  4736.000000   \n",
       "mean      0.493042     0.49147     0.487756     0.494300     0.487503   \n",
       "std       0.288491     0.28590     0.288671     0.288194     0.286987   \n",
       "min       0.000000     0.00000     0.000000     0.000000     0.000000   \n",
       "25%       0.244000     0.24600     0.238000     0.248000     0.244000   \n",
       "50%       0.490000     0.48600     0.480000     0.490000     0.480000   \n",
       "75%       0.740000     0.73600     0.728000     0.740000     0.728500   \n",
       "max       0.998000     0.99800     0.998000     0.998000     0.998000   \n",
       "\n",
       "               497          498          499    500    501  \n",
       "count  4711.000000  4689.000000  4663.000000  1.000  1.000  \n",
       "mean      0.490444     0.496392     0.488711  0.288  0.798  \n",
       "std       0.288758     0.285028     0.285562    NaN    NaN  \n",
       "min       0.000000     0.000000     0.000000  0.288  0.798  \n",
       "25%       0.242000     0.252000     0.248000  0.288  0.798  \n",
       "50%       0.486000     0.492000     0.484000  0.288  0.798  \n",
       "75%       0.738000     0.734000     0.725000  0.288  0.798  \n",
       "max       0.998000     0.998000     0.998000  0.288  0.798  \n",
       "\n",
       "[8 rows x 502 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>492</th>\n      <th>493</th>\n      <th>494</th>\n      <th>495</th>\n      <th>496</th>\n      <th>497</th>\n      <th>498</th>\n      <th>499</th>\n      <th>500</th>\n      <th>501</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>6548.000000</td>\n      <td>6548.000000</td>\n      <td>6548.000000</td>\n      <td>6547.000000</td>\n      <td>6547.000000</td>\n      <td>6547.000000</td>\n      <td>6546.000000</td>\n      <td>6545.000000</td>\n      <td>6545.000000</td>\n      <td>6544.000000</td>\n      <td>...</td>\n      <td>4837.000000</td>\n      <td>4813.00000</td>\n      <td>4785.000000</td>\n      <td>4754.000000</td>\n      <td>4736.000000</td>\n      <td>4711.000000</td>\n      <td>4689.000000</td>\n      <td>4663.000000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.496664</td>\n      <td>0.495801</td>\n      <td>0.496177</td>\n      <td>0.493760</td>\n      <td>0.499219</td>\n      <td>0.501779</td>\n      <td>0.489562</td>\n      <td>0.497996</td>\n      <td>0.497606</td>\n      <td>0.493351</td>\n      <td>...</td>\n      <td>0.493042</td>\n      <td>0.49147</td>\n      <td>0.487756</td>\n      <td>0.494300</td>\n      <td>0.487503</td>\n      <td>0.490444</td>\n      <td>0.496392</td>\n      <td>0.488711</td>\n      <td>0.288</td>\n      <td>0.798</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.285253</td>\n      <td>0.289804</td>\n      <td>0.286167</td>\n      <td>0.287957</td>\n      <td>0.288914</td>\n      <td>0.285597</td>\n      <td>0.285544</td>\n      <td>0.286654</td>\n      <td>0.289569</td>\n      <td>0.288133</td>\n      <td>...</td>\n      <td>0.288491</td>\n      <td>0.28590</td>\n      <td>0.288671</td>\n      <td>0.288194</td>\n      <td>0.286987</td>\n      <td>0.288758</td>\n      <td>0.285028</td>\n      <td>0.285562</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.288</td>\n      <td>0.798</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.252000</td>\n      <td>0.246000</td>\n      <td>0.250000</td>\n      <td>0.246000</td>\n      <td>0.248000</td>\n      <td>0.256000</td>\n      <td>0.244000</td>\n      <td>0.254000</td>\n      <td>0.244000</td>\n      <td>0.248000</td>\n      <td>...</td>\n      <td>0.244000</td>\n      <td>0.24600</td>\n      <td>0.238000</td>\n      <td>0.248000</td>\n      <td>0.244000</td>\n      <td>0.242000</td>\n      <td>0.252000</td>\n      <td>0.248000</td>\n      <td>0.288</td>\n      <td>0.798</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.500000</td>\n      <td>0.492000</td>\n      <td>0.490000</td>\n      <td>0.490000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.485000</td>\n      <td>0.496000</td>\n      <td>0.500000</td>\n      <td>0.488000</td>\n      <td>...</td>\n      <td>0.490000</td>\n      <td>0.48600</td>\n      <td>0.480000</td>\n      <td>0.490000</td>\n      <td>0.480000</td>\n      <td>0.486000</td>\n      <td>0.492000</td>\n      <td>0.484000</td>\n      <td>0.288</td>\n      <td>0.798</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.740000</td>\n      <td>0.748000</td>\n      <td>0.742000</td>\n      <td>0.744000</td>\n      <td>0.752000</td>\n      <td>0.748000</td>\n      <td>0.736000</td>\n      <td>0.746000</td>\n      <td>0.748000</td>\n      <td>0.742000</td>\n      <td>...</td>\n      <td>0.740000</td>\n      <td>0.73600</td>\n      <td>0.728000</td>\n      <td>0.740000</td>\n      <td>0.728500</td>\n      <td>0.738000</td>\n      <td>0.734000</td>\n      <td>0.725000</td>\n      <td>0.288</td>\n      <td>0.798</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>...</td>\n      <td>0.998000</td>\n      <td>0.99800</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.998000</td>\n      <td>0.288</td>\n      <td>0.798</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 502 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "# sample_df = car_df\n",
    "# sample_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "# np.any(sample_df.isin([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6554,)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "# np.shape(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df[df[‘Name’]==’Donna’].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 90,  63, 126, 393,  51, 341, 191, 308, 336, 362, 319, 296, 392,\n",
       "       495, 255, 409,  63, 383, 270,  23, 368,  37, 160, 496, 405,  52,\n",
       "       304,  21, 182, 379,   5, 225,  12, 113,  95, 173, 212, 134, 389,\n",
       "       382,   7, 400, 370, 236, 254, 238,  12, 146, 219, 481, 441,  76,\n",
       "        62, 342, 257, 473, 185, 221,  73,  47, 254, 369, 163,  54, 432,\n",
       "       177, 471, 132, 418,  13, 370, 277, 419,  95,  65, 300, 348, 259,\n",
       "       190, 497, 134,  73, 233, 263,  33, 453, 170, 208,  65, 435,  19,\n",
       "       253, 202,   4, 111, 263, 419, 107, 250, 364, 138, 263, 263, 236,\n",
       "        85, 228, 252, 369, 172,  61,  93, 123,  20, 484, 481, 283, 484,\n",
       "        20, 168, 459,  40, 238, 201, 292,  85, 422, 399,  53, 284, 241,\n",
       "       319, 139, 135, 401, 367, 231,  12, 254, 497, 246, 280, 405, 382,\n",
       "       226, 191, 130, 169,  43,  75, 173, 232, 231,  47, 160, 454, 495,\n",
       "       111,  65,  11, 393, 478, 455,  76,  50, 490,  74, 285, 133,  78,\n",
       "        22, 419, 299, 324, 307, 320, 264,  67, 105,  59, 332,  90, 151,\n",
       "       325, 379,  56, 164, 252,   2, 220, 128,  54, 233, 328, 197,  94,\n",
       "       426, 234,  11, 224, 283, 252,  95, 255,  75, 282, 141,  43, 116,\n",
       "       248, 163,  95, 121, 496, 382, 254, 486, 297, 135, 342, 110,   0,\n",
       "       292,  20,  20,  19, 123,  63, 220, 203, 387, 249, 123, 354, 422,\n",
       "       446, 292, 469, 383, 158,  51, 167, 473, 324, 258, 253, 122, 300,\n",
       "       291, 416, 436, 258,   6, 187, 459, 160, 348, 197, 329,  25,  65,\n",
       "       106, 389, 320, 262, 400, 459, 300, 267, 378, 346, 385, 285, 473,\n",
       "        44,  44, 421, 354, 352, 263, 281, 249, 234, 336, 300, 364, 300,\n",
       "       297, 281,  42, 305, 121, 161, 281, 163, 308,  51, 201, 465, 436,\n",
       "       432, 324, 327, 258, 158, 190, 476, 382, 354, 354, 259, 105,  12,\n",
       "       481, 405, 248, 421, 387, 166,  63, 156, 186,   6, 258, 479, 483,\n",
       "        81, 473, 352,  61,  11, 388, 249,  11,  80, 275, 281,  92, 180,\n",
       "       249, 117, 459, 250, 246, 142, 180, 429, 375, 498, 227,  77, 397,\n",
       "       336,  72, 296, 294, 390, 480, 181,  77, 248, 190, 170,  77, 280,\n",
       "       217, 220,  53, 190, 249, 477, 348, 356, 218, 244, 439, 397, 476,\n",
       "        90,  51, 366, 402, 470, 359, 112,  77, 397, 453, 208, 151, 280,\n",
       "        53, 486, 366, 180, 182, 284, 263, 109, 309, 146, 263, 437, 246,\n",
       "       140,  18, 355,  48, 366, 445,  87, 170, 162, 207, 336, 186, 227,\n",
       "        38, 375, 148,  29, 172,  82, 472, 166, 421, 269, 479, 135, 323,\n",
       "        59, 366, 344, 460, 445,  20, 263, 432,  49, 348, 409, 109,  71,\n",
       "        40, 424, 420, 433, 469, 200, 108,  46, 302, 155, 286, 160, 341,\n",
       "        20, 141, 249, 450, 165, 368, 348, 209, 466, 142,  77, 343, 337,\n",
       "       328, 196, 260,  51,  20, 264, 266, 264, 179, 180, 342, 417, 433,\n",
       "        20, 445, 200, 386, 316, 143, 283, 224, 155, 488, 145, 348,  67,\n",
       "       161, 302, 108, 234, 302, 201])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "# img_cluster[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6548, 500)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "# np.shape(hist_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 55,  45, 107,  86, 150, 242,  94, 115, 113, 194, 219, 195, 113,\n",
       "       126,  53, 112, 126, 239,  21, 175, 252, 155,  52, 238,  59,  46,\n",
       "       143, 130,  37, 247, 246, 246])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# np.array(pickle_file[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(32, 500)\n[ 90  63 126 393  51 341 191 308 336 362 319 296 392 495 255 409  63 383\n 270  23 368  37 160 496 405  52 304  21 182 379   5 225  12 113  95 173\n 212 134 389 382   7 400 370 236 254 238  12 146 219 481 441  76  62 342\n 257 473 185 221  73  47 254 369 163  54 432 177 471 132 418  13 370 277\n 419  95  65 300 348 259 190 497 134  73 233 263  33 453 170 208  65 435\n  19 253 202   4 111 263 419 107 250 364 138 263 263 236  85 228 252 369\n 172  61  93 123  20 484 481 283 484  20 168 459  40 238 201 292  85 422\n 399  53 284 241 319 139 135 401 367 231  12 254 497 246 280 405 382 226\n 191 130 169  43  75 173 232 231  47 160 454 495 111  65  11 393 478 455\n  76  50 490  74 285 133  78  22 419 299 324 307 320 264  67 105  59 332\n  90 151 325 379  56 164 252   2 220 128  54 233 328 197  94 426 234  11\n 224 283 252  95 255  75 282 141  43 116 248 163  95 121 496 382 254 486\n 297 135 342 110   0 292  20  20  19 123  63 220 203 387 249 123 354 422\n 446 292 469 383 158  51 167 473 324 258 253 122 300 291 416 436 258   6\n 187 459 160 348 197 329  25  65 106 389 320 262 400 459 300 267 378 346\n 385 285 473  44  44 421 354 352 263 281 249 234 336 300 364 300 297 281\n  42 305 121 161 281 163 308  51 201 465 436 432 324 327 258 158 190 476\n 382 354 354 259 105  12 481 405 248 421 387 166  63 156 186   6 258 479\n 483  81 473 352  61  11 388 249  11  80 275 281  92 180 249 117 459 250\n 246 142 180 429 375 498 227  77 397 336  72 296 294 390 480 181  77 248\n 190 170  77 280 217 220  53 190 249 477 348 356 218 244 439 397 476  90\n  51 366 402 470 359 112  77 397 453 208 151 280  53 486 366 180 182 284\n 263 109 309 146 263 437 246 140  18 355  48 366 445  87 170 162 207 336\n 186 227  38 375 148  29 172  82 472 166 421 269 479 135 323  59 366 344\n 460 445  20 263 432  49 348 409 109  71  40 424 420 433 469 200 108  46\n 302 155 286 160 341  20 141 249 450 165 368 348 209 466 142  77 343 337\n 328 196 260  51  20 264 266 264 179 180 342 417 433  20 445 200 386 316\n 143 283 224 155 488 145 348  67 161 302 108 234 302 201]\n"
     ]
    }
   ],
   "source": [
    "#  desc = np.array(pickle_file[0])\n",
    "# #  print(np.shape(desc.T))\n",
    "#  print(kmeans_batch.predict(desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}