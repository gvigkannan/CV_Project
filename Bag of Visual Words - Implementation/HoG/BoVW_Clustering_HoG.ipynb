{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual BoW - Clustering using MiniBatchKMeans for HoG Features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from sys import getsizeof\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = os.getcwd()\n",
    "train_dir = \"\\\\\".join((path_dir, \"HoG_Features\", \"HoG_Train\"))\n",
    "train_splits = os.listdir(train_dir)\n",
    "car_splits = train_splits[:4]\n",
    "noise_splits = train_splits[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['noise_train_0_1694.txt',\n",
       " 'noise_train_1694_3388.txt',\n",
       " 'noise_train_3388_5082.txt',\n",
       " 'noise_train_5082_6776.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "noise_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exacting all the different splits of the car dataset - for train!\n",
    "split_path = \"\\\\\".join((train_dir, car_splits[0]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_0 = pickle.load(file_name)\n",
    "\n",
    "# split_path = \"\\\\\".join((train_dir, car_splits[1]))\n",
    "# with open(split_path, 'rb') as file_name:\n",
    "#     car_pickle_1 = pickle.load(file_name)\n",
    "\n",
    "# split_path = \"\\\\\".join((train_dir, car_splits[2]))\n",
    "# with open(split_path, 'rb') as file_name:\n",
    "#     car_pickle_2 = pickle.load(file_name)\n",
    "\n",
    "# split_path = \"\\\\\".join((train_dir, car_splits[3]))\n",
    "# with open(split_path, 'rb') as file_name:\n",
    "#     car_pickle_3 = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exacting all the different splits of the noise dataset - for train!\n",
    "split_path = \"\\\\\".join((train_dir, noise_splits[0]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_0 = pickle.load(file_name)\n",
    "\n",
    "# split_path = \"\\\\\".join((train_dir, noise_splits[1]))\n",
    "# with open(split_path, 'rb') as file_name:\n",
    "#     noise_pickle_1 = pickle.load(file_name)\n",
    "\n",
    "# split_path = \"\\\\\".join((train_dir, noise_splits[2]))\n",
    "# with open(split_path, 'rb') as file_name:\n",
    "#     noise_pickle_2 = pickle.load(file_name)\n",
    "\n",
    "# split_path = \"\\\\\".join((train_dir, noise_splits[3]))\n",
    "# with open(split_path, 'rb') as file_name:\n",
    "#     noise_pickle_3 = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The Integraty of the Dataset is maintained but the display is all over the place. \n",
    "# # Long live pickle! But too much space!\n",
    "# desc_0 = np.asarray(desc_0)\n",
    "# label_0 = np.array(label_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each image descriptor has a shape (m, n, 32). \n",
    "# Where m = number of descriptors\n",
    "# n = number of descriptor vectors for each descriptor\n",
    "# 32 - number for each vector\n",
    "\n",
    "# We need to reshape such that we get (m x n, 32)\n",
    "# np.shape(car_pickle_0[0][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1638,)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "np.shape(car_pickle_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the pickle_file into four different parts!\n",
    "size = len(car_pickle_0)\n",
    "car_pickle_0_0 = car_pickle_0[:int(size*0.25)]\n",
    "car_pickle_0_1 = car_pickle_0[int(size*0.25):int(size*0.5)]\n",
    "car_pickle_0_2 = car_pickle_0[int(size*0.5):int(size*0.75)]\n",
    "car_pickle_0_3 = car_pickle_0[int(size*0.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the pickle_file into four different parts!\n",
    "size = len(noise_pickle_0)\n",
    "noise_pickle_0_0 = noise_pickle_0[:int(size*0.25)]\n",
    "noise_pickle_0_1 = noise_pickle_0[int(size*0.25):int(size*0.5)]\n",
    "noise_pickle_0_2 = noise_pickle_0[int(size*0.5):int(size*0.75)]\n",
    "noise_pickle_0_3 = noise_pickle_0[int(size*0.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_descriptor(pickle_file):\n",
    "    start_time = time.time()\n",
    "    print(\"Before Extraction: \\t\", np.shape(pickle_file))\n",
    "    desc_arr = []\n",
    "    for l in pickle_file:\n",
    "        if len(l) != 0:\n",
    "            for desc in l:\n",
    "                desc_arr.append(desc)\n",
    "\n",
    "    print(\"After Extraction: \\t\", np.shape(desc_arr))\n",
    "    print(\"Time in sec\", time.time() - start_time)\n",
    "    return(desc_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before Extraction: \t (409,)\n",
      "After Extraction: \t (31952608,)\n",
      "Time in sec 6.866515636444092\n",
      "Before Extraction: \t (423,)\n",
      "After Extraction: \t (47342528,)\n",
      "Time in sec 10.910331964492798\n",
      "Before Extraction: \t (410,)\n",
      "After Extraction: \t (25809280,)\n",
      "Time in sec 5.703147888183594\n",
      "Before Extraction: \t (424,)\n",
      "After Extraction: \t (51188224,)\n",
      "Time in sec 14.268203258514404\n"
     ]
    }
   ],
   "source": [
    "#Using a function to reshape the images based on descriptor length for clustering!\n",
    "car_desc_0_0 = reshape_descriptor(car_pickle_0_0)\n",
    "noise_desc_0_0 = reshape_descriptor(noise_pickle_0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before Extraction: \t (410,)\n",
      "After Extraction: \t (25809280,)\n",
      "Time in sec 5.3537163734436035\n",
      "Before Extraction: \t (424,)\n",
      "After Extraction: \t (51188224,)\n",
      "Time in sec 11.232972383499146\n"
     ]
    }
   ],
   "source": [
    "car_desc_0_1 = reshape_descriptor(car_pickle_0_1)\n",
    "noise_desc_0_1 = reshape_descriptor(noise_pickle_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "264748592"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "getsizeof(car_desc_0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car_desc_1 = reshape_descriptor(car_pickle_1)\n",
    "# noise_desc_1 = reshape_descriptor(noise_pickle_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car_desc_2 = reshape_descriptor(car_pickle_2)\n",
    "# noise_desc_2 = reshape_descriptor(noise_pickle_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car_desc_3 = reshape_descriptor(car_pickle_3)\n",
    "# noise_desc_3 = reshape_descriptor(noise_pickle_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(car_0))\n",
    "# i = 0\n",
    "# car_desc_0 = []\n",
    "# shape_num = []\n",
    "# for l in car_pickle_0:\n",
    "#     if len(l) > 0:\n",
    "#         for desc in l:\n",
    "#             car_desc_0.append(desc)\n",
    "        # car_temp = np.ravel(l).astype(float)\n",
    "        # shape_num.append(np.shape(car_temp))\n",
    "        # car0.append(np.asarray(car_temp).astype(float))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(778081, 32)\n"
     ]
    }
   ],
   "source": [
    "# print(np.shape(car_desc_0)) # Holy smokes! # (778081, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Extract Noise Features\n",
    "# print(np.shape(noise_0))\n",
    "# i = 0\n",
    "# noise_desc_0 = []\n",
    "# shape_num = []\n",
    "# for l in noise_pickle_0:\n",
    "#     if len(l) > 0:\n",
    "#         for desc in l:\n",
    "#             noise_desc_0.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(noise_desc_0)) # (778081, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape_2num = []\n",
    "# car_ravel_0 = []\n",
    "# for l in car0:\n",
    "#     car_temp = np.ravel(l)\n",
    "#     shape_2num.append(np.shape(car_temp))\n",
    "#     car_ravel_0.append(np.asarray(car_temp).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car_reshape = np.ravel(car_ravel_0)\n",
    "# car_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_no = 500\n",
    "iters = 1000\n",
    "bs = 32\n",
    "rs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "# Let's change the number of cluster and random state to see how it varies!\n",
    "kmeans_batch = MiniBatchKMeans( n_clusters = cluster_no,\n",
    "                                max_iter = iters,\n",
    "                                batch_size = bs,\n",
    "                                random_state = rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MiniBatchKMeans Clustering using Partial Fits!\n",
    "# Vary the different arrangement of fitting to see how it works\n",
    "# Change tolerances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  154.38468313217163\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_0)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  25.005149126052856\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_1)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  35.10488438606262\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_2)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  34.30988669395447\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_3)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  35.165987968444824\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_0)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  35.18739414215088\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_1)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  34.85325217247009\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_2)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  35.03973865509033\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_3)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KMeans_c_800_b_32_rs_5_.sav\n"
     ]
    }
   ],
   "source": [
    "name = \"_\".join((\"KMeans\", \"c\", str(cluster_no), \"b\", str(bs), \"rs\", str(rs), \".sav\"))\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping the trained model!\n",
    "pickle.dump(kmeans_batch, open(name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run it all!"
   ]
  }
 ]
}