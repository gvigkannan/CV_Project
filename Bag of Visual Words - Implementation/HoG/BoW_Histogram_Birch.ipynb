{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans, Birch\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = os.getcwd()\n",
    "train_dir = \"\\\\\".join((path_dir, \"ORB_Dataset\", \"train\"))\n",
    "train_files = os.listdir(train_dir)\n",
    "train_car_dir = \"\\\\\".join((train_dir, train_files[0]))\n",
    "train_noise_dir = \"\\\\\".join((train_dir, train_files[1]))\n",
    "# noise_splits = os.listdir(train_noise_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exacting all the different splits of the car dataset - for train!\n",
    "split_path = \"\\\\\".join((train_car_dir, car_splits[0]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_0 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((train_car_dir, car_splits[1]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_1 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((train_car_dir, car_splits[2]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_2 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((train_car_dir, car_splits[3]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    car_pickle_3 = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exacting all the different splits of the noise dataset - for train!\n",
    "split_path = \"\\\\\".join((train_noise_dir, noise_splits[0]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_0 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((train_noise_dir, noise_splits[1]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_1 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((train_noise_dir, noise_splits[2]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_2 = pickle.load(file_name)\n",
    "\n",
    "split_path = \"\\\\\".join((train_noise_dir, noise_splits[3]))\n",
    "with open(split_path, 'rb') as file_name:\n",
    "    noise_pickle_3 = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "22.255486726760864\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "car_descriptors = []\n",
    "with open(train_car_dir, 'r') as file_name:\n",
    "    car_descriptors = json.load(file_name)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25.508760690689087\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "noise_descriptors = []\n",
    "with open(train_noise_dir, 'r') as file_name:\n",
    "    noise_descriptors = json.load(file_name)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_descriptor(pickle_file):\n",
    "    print(\"Before Extraction: \\t\", np.shape(pickle_file))\n",
    "    desc_arr = []\n",
    "    for l in pickle_file:\n",
    "        if len(l) != 0:\n",
    "            for desc in l:\n",
    "                desc_arr.append(desc)\n",
    "\n",
    "    print(\"After Extraction: \\t\", np.shape(desc_arr))\n",
    "\n",
    "    return(desc_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before Extraction: \t (1638,)\n",
      "After Extraction: \t (778081, 32)\n",
      "Before Extraction: \t (1694,)\n",
      "After Extraction: \t (836822, 32)\n",
      "\n",
      "\n",
      "Before Extraction: \t (1639,)\n",
      "After Extraction: \t (773890, 32)\n",
      "Before Extraction: \t (1694,)\n",
      "After Extraction: \t (833706, 32)\n",
      "\n",
      "\n",
      "Before Extraction: \t (1638,)\n",
      "After Extraction: \t (773428, 32)\n",
      "Before Extraction: \t (1694,)\n",
      "After Extraction: \t (837641, 32)\n",
      "\n",
      "\n",
      "Before Extraction: \t (1639,)\n",
      "After Extraction: \t (771932, 32)\n",
      "Before Extraction: \t (1694,)\n",
      "After Extraction: \t (837941, 32)\n"
     ]
    }
   ],
   "source": [
    "#Using a function to reshape the images based on descriptor length for clustering!\n",
    "# car_desc = reshape_descriptor(car_pickle_0)\n",
    "# noise_desc = reshape_descriptor(noise_pickle_0)\n",
    "# car_desc_0 = reshape_descriptor(car_pickle_0)\n",
    "# noise_desc_0 = reshape_descriptor(noise_pickle_0)\n",
    "# print(\"\\n\")\n",
    "# car_desc_1 = reshape_descriptor(car_pickle_1)\n",
    "# noise_desc_1 = reshape_descriptor(noise_pickle_1)\n",
    "# print(\"\\n\")\n",
    "# car_desc_2 = reshape_descriptor(car_pickle_2)\n",
    "# noise_desc_2 = reshape_descriptor(noise_pickle_2)\n",
    "# print(\"\\n\")\n",
    "# car_desc_3 = reshape_descriptor(car_pickle_3)\n",
    "# noise_desc_3 = reshape_descriptor(noise_pickle_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_no = 800\n",
    "iters = 1000\n",
    "bs = 32\n",
    "rs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "# Let's change the number of cluster and random state to see how it varies!\n",
    "kmeans_batch = MiniBatchKMeans( n_clusters = cluster_no,\n",
    "                                max_iter = iters,\n",
    "                                batch_size = bs,\n",
    "                                random_state = rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  136.79235863685608\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_0)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  21.309828281402588\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_1)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  21.025275707244873\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_2)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  22.184691429138184\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(car_desc_3)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  24.1213059425354\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_0)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  25.029911279678345\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_1)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  23.592458963394165\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_2)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time:  24.207860469818115\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "kmeans_batch.partial_fit(noise_desc_3)\n",
    "print(\"Time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KMeans_c_800_b_32_rs_5_v1.sav\n"
     ]
    }
   ],
   "source": [
    "name = \"_\".join((\"KMeans\", \"c\", str(cluster_no), \"b\", str(bs), \"rs\", str(rs), \"v1.sav\"))\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping the trained model!\n",
    "pickle.dump(kmeans_batch, open(name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "brc_batch = Birch(n_clusters=800)\n",
    "#Needs 2.18TB!"
   ]
  }
 ]
}