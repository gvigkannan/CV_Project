{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time \n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'i:\\\\WPI\\\\Fall 2020\\\\Computer Vision - RBE 549\\\\CV Project\\\\Dataset\\\\cars_train\\\\Dimensionality Experiments'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "path_dir = os.getcwd()\n",
    "path_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_dir = '\\\\'.join((path_dir, \"Images\"))\n",
    "car_names = os.listdir(fold_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3873605728149414\n"
     ]
    }
   ],
   "source": [
    "#Without Resizing:\n",
    "start_time = time.time()\n",
    "\n",
    "img_cars = []\n",
    "for file_n in car_names:\n",
    "    file_name = \"\\\\\".join((fold_dir, file_n))\n",
    "    img = np.array(Image.open(file_name).convert('LA'))\n",
    "    #img_grey = tf.image.resize(img, (256,256),preserve_aspect_ratio=False).numpy()\n",
    "    img_cars.append(img[:,:,0])\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.429199457168579\n"
     ]
    }
   ],
   "source": [
    "#For Image Resizing:\n",
    "start_time = time.time()\n",
    "\n",
    "img_cars_resized = []\n",
    "for file_n in car_names:\n",
    "    file_name = \"\\\\\".join((fold_dir, file_n))\n",
    "    img = np.array(Image.open(file_name).convert('LA'))\n",
    "    img_grey = cv2.resize(img, (256,256)) \n",
    "    img_cars_resized.append(img_grey[:,:,0])\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.4561100006103516\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "freak = cv2.xfeatures2d.FREAK_create()\n",
    "desc_freak_normal = []\n",
    "kp_normal = []\n",
    "for img in img_cars:\n",
    "    img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    fast = cv2.FastFeatureDetector_create(12)\n",
    "    kp = fast.detect(img, None)\n",
    "    kp, des = freak.compute(img, kp)\n",
    "\n",
    "    if not des is None:   \n",
    "        desc_freak_normal.append(des)\n",
    "    # if not kp is None:\n",
    "    #     kp_normal.append(kp)\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.39896368980407715\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "freak = cv2.xfeatures2d.FREAK_create()\n",
    "desc_freak_scaled = []\n",
    "kp_scaled = []\n",
    "for img in img_cars_resized:\n",
    "    img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    fast = cv2.FastFeatureDetector_create(12)\n",
    "    kp = fast.detect(img, None)\n",
    "    kp, des = freak.compute(img, kp)\n",
    "\n",
    "    if not des is None:   \n",
    "        desc_freak_scaled.append(des)\n",
    "    # if not kp is None:\n",
    "    #     kp_normal.append(kp)\n",
    "\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_scaled = []\n",
    "scaled_shape = []\n",
    "flatten_scaled_shape = []\n",
    "for desc in desc_freak_scaled:\n",
    "    flatten_scaled.append(np.resize(desc, (-1)))\n",
    "    scaled_shape.append(np.shape(desc)[0])\n",
    "    flatten_scaled_shape.append(np.shape(np.resize(desc, (-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(412, 64)\n(1668, 64)\n(1024, 64)\n(621, 64)\n(111, 64)\n"
     ]
    }
   ],
   "source": [
    "#Typical Descriptor for Original Image\n",
    "print(np.shape(desc_freak_normal[0]))\n",
    "print(np.shape(desc_freak_normal[1]))\n",
    "print(np.shape(desc_freak_normal[2]))\n",
    "print(np.shape(desc_freak_normal[3]))\n",
    "print(np.shape(desc_freak_normal[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(211, 64)\n(527, 64)\n(300, 64)\n(148, 64)\n(151, 64)\n"
     ]
    }
   ],
   "source": [
    "#Typical Descriptor for Scaled Images\n",
    "print(np.shape(desc_freak_scaled[0]))\n",
    "print(np.shape(desc_freak_scaled[1]))\n",
    "print(np.shape(desc_freak_scaled[2]))\n",
    "print(np.shape(desc_freak_scaled[3]))\n",
    "print(np.shape(desc_freak_scaled[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_original = []\n",
    "normal_shape = []\n",
    "flatten_normal_shape = []\n",
    "for desc in desc_freak_normal:\n",
    "    flatten_original.append(np.resize(desc, (-1)))\n",
    "    normal_shape.append(np.shape(np.resize(desc, (-1))))\n",
    "    flatten_normal_shape.append(np.shape(np.resize(desc, (-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Maximum Number of Descriptors - Original:  470911\nMinimum Number of Descriptors - Original:  511\n\n[   511   7103   8383   9919  12031  13183  13503  14271  14719  17023\n  17535  17983  18431  18751  20095  20671  23807  23935  25087  25151\n  26367  28031  28735  29375  29567  29631  30655  30719  32127  32255\n  32511  32639  33023  35455  35647  35839  36543  36991  37759  38719\n  38783  38911  38975  39295  39743  41023  42367  42943  43263  46527\n  47359  48127  48191  48255  49407  49983  51263  53375  54527  55935\n  56639  56703  57407  60351  63487  64575  65535  65983  67263  68927\n  70975  71487  73471  74943  75135  76863  79615  79935  81151  87167\n  87487  88575  90815  93119  98111 100223 100351 101119 106751 108287\n 110911 112319 115135 122879 131583 146239 276415 307391 440191 470911]\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "count, values = np.unique(normal_shape, return_counts=True)\n",
    "print(\"Maximum Number of Descriptors - Original: \", np.max(normal_shape))\n",
    "print(\"Minimum Number of Descriptors - Original: \",np.min(normal_shape))\n",
    "print()\n",
    "print(count)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Maximum Number of Descriptors - Scaled:  790\nMinimum Number of Descriptors - Scaled:  72\n\n[ 72 123 133 136 148 151 155 161 164 167 174 176 178 179 180 182 183 188\n 189 206 208 211 213 223 226 228 229 232 233 245 248 250 252 259 270 273\n 282 288 291 296 300 305 306 308 309 311 313 314 315 323 329 332 334 335\n 341 342 343 348 358 364 367 369 372 373 376 382 389 402 405 414 421 429\n 439 443 448 455 462 479 488 490 507 514 516 523 527 560 566 567 627 761\n 790]\n[1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 2\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "count, values = np.unique(scaled_shape, return_counts=True)\n",
    "print(\"Maximum Number of Descriptors - Scaled: \", np.max(scaled_shape))\n",
    "print(\"Minimum Number of Descriptors - Scaled: \", np.min(scaled_shape))\n",
    "print()\n",
    "print(count)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Maximum Number of Features - Scaled-Flatten:  50559\nMinimum Number of Features - Scaled-Flatten:  4607\n\n[ 4607  7871  8511  8703  9471  9663  9919 10303 10495 10687 11135 11263\n 11391 11455 11519 11647 11711 12031 12095 13183 13311 13503 13631 14271\n 14463 14591 14655 14847 14911 15679 15871 15999 16127 16575 17279 17471\n 18047 18431 18623 18943 19199 19519 19583 19711 19775 19903 20031 20095\n 20159 20671 21055 21247 21375 21439 21823 21887 21951 22271 22911 23295\n 23487 23615 23807 23871 24063 24447 24895 25727 25919 26495 26943 27455\n 28095 28351 28671 29119 29567 30655 31231 31359 32447 32895 33023 33471\n 33727 35839 36223 36287 40127 48703 50559]\n[1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 2\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "count, values = np.unique(flatten_scaled_shape, return_counts=True)\n",
    "print(\"Maximum Number of Features - Scaled-Flatten: \", np.max(flatten_scaled_shape))\n",
    "print(\"Minimum Number of Features - Scaled-Flatten: \", np.min(flatten_scaled_shape))\n",
    "print()\n",
    "print(count)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Maximum Number of Features - Original-Flatten:  470911\nMinimum Number of Features - Original-Flatten:  511\n\n[   511   7103   8383   9919  12031  13183  13503  14271  14719  17023\n  17535  17983  18431  18751  20095  20671  23807  23935  25087  25151\n  26367  28031  28735  29375  29567  29631  30655  30719  32127  32255\n  32511  32639  33023  35455  35647  35839  36543  36991  37759  38719\n  38783  38911  38975  39295  39743  41023  42367  42943  43263  46527\n  47359  48127  48191  48255  49407  49983  51263  53375  54527  55935\n  56639  56703  57407  60351  63487  64575  65535  65983  67263  68927\n  70975  71487  73471  74943  75135  76863  79615  79935  81151  87167\n  87487  88575  90815  93119  98111 100223 100351 101119 106751 108287\n 110911 112319 115135 122879 131583 146239 276415 307391 440191 470911]\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "count, values = np.unique(flatten_normal_shape, return_counts=True)\n",
    "print(\"Maximum Number of Features - Original-Flatten: \",np.max(flatten_normal_shape))\n",
    "print(\"Minimum Number of Features - Original-Flatten: \",np.min(flatten_normal_shape))\n",
    "print()\n",
    "print(count)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}