{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from random import randrange\n",
    "import pickle\n",
    "import json\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shuffled_data():\n",
    "    \"\"\"\n",
    "    Load dataset from pickle file and split features and labels\n",
    "    returns (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(\"sample_train_img.pkl\", 'rb') as f_name:\n",
    "        train_img = pickle.load(f_name)\n",
    "        train_img /= 255.0\n",
    "\n",
    "    with open(\"sample_train_label.pkl\", 'rb') as f_name:\n",
    "        train_label = pickle.load(f_name)\n",
    "        train_label = tf.keras.utils.to_categorical(train_label.astype('int'))\n",
    "        \n",
    "    \n",
    "    with open(\"sample_test_img.pkl\", 'rb') as f_name:\n",
    "        test_img = pickle.load(f_name)\n",
    "        test_img /= 255.0\n",
    "\n",
    "    with open(\"sample_test_label.pkl\", 'rb') as f_name:\n",
    "        test_label = pickle.load(f_name)\n",
    "        test_label = tf.keras.utils.to_categorical(test_label.astype('int'))\n",
    "\n",
    "    #Shuffling Images!\n",
    "\n",
    "    permute_train = np.random.permutation(len(train_label))\n",
    "    permute_test = np.random.permutation(len(test_label))\n",
    "\n",
    "    train_img = train_img[permute_train]\n",
    "    train_label = train_label[permute_train]\n",
    "\n",
    "    test_img = train_img[permute_train]\n",
    "    test_label = train_label[permute_train]\n",
    "\n",
    "    return(train_img, test_img, train_label, test_label)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(image_height = 256, image_width = 256, image_channel=1, class_count = 2):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(   filters = 64, \n",
    "                        kernel_size=(9,9), \n",
    "                        strides = 2, \n",
    "                        activation='relu', \n",
    "                        kernel_initializer='he_uniform', \n",
    "                        input_shape = (image_height, image_width, image_channel)))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #Taking the maximum in that subregion using MaxPooling\n",
    "    model.add(MaxPool2D(    pool_size = (3, 3),\n",
    "                            strides = (2, 2)))\n",
    "    #he_uniform: Draw samples from uniform distributions within range of limits created with units\n",
    "\n",
    "    model.add(Conv2D(   filters = 256, \n",
    "                        kernel_size=(5,5), \n",
    "                        strides = 1, \n",
    "                        activation='relu', \n",
    "                        kernel_initializer='he_uniform'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(   filters = 128, \n",
    "                        kernel_size=(3,3), \n",
    "                        strides = 1, \n",
    "                        activation='relu', \n",
    "                        kernel_initializer='he_uniform'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(    pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(    units = 4096, \n",
    "                        activation='relu',              \n",
    "                        kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.5))   \n",
    "\n",
    "    model.add(Dense(    units = 4096, \n",
    "                        activation='relu', \n",
    "                        kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.5))   \n",
    "\n",
    "    model.add(Dense(    units = 2, \n",
    "                        activation='softmax'))\n",
    "\n",
    "    #Compiling:\n",
    "    opt = SGD(lr = 0.001, momentum = 0.9)\n",
    "    model.compile(  optimizer=opt, \n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN_model()\n",
    "train_x, test_x, train_y, test_y = load_shuffled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'CNN_3Layer_v2_1.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/10\n",
      "160/180 [=========================>....] - ETA: 2s - loss: 2.0320 - accuracy: 0.5437\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50000, saving model to best_model_trial_3.h5\n",
      "180/180 [==============================] - 24s 135ms/sample - loss: 1.9149 - accuracy: 0.5611 - val_loss: 0.8424 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "160/180 [=========================>....] - ETA: 1s - loss: 0.9258 - accuracy: 0.7875\n",
      "Epoch 00002: val_accuracy improved from 0.50000 to 0.52778, saving model to best_model_trial_3.h5\n",
      "180/180 [==============================] - 23s 128ms/sample - loss: 0.9719 - accuracy: 0.7833 - val_loss: 0.9750 - val_accuracy: 0.5278\n",
      "Epoch 3/10\n",
      "160/180 [=========================>....] - ETA: 1s - loss: 0.8684 - accuracy: 0.8062\n",
      "Epoch 00003: val_accuracy did not improve from 0.52778\n",
      "180/180 [==============================] - 21s 118ms/sample - loss: 0.8636 - accuracy: 0.8056 - val_loss: 1.9096 - val_accuracy: 0.5111\n",
      "Epoch 4/10\n",
      "160/180 [=========================>....] - ETA: 1s - loss: 0.1512 - accuracy: 0.9438\n",
      "Epoch 00004: val_accuracy improved from 0.52778 to 0.58333, saving model to best_model_trial_3.h5\n",
      "180/180 [==============================] - 24s 131ms/sample - loss: 0.1828 - accuracy: 0.9222 - val_loss: 1.2016 - val_accuracy: 0.5833\n",
      "Epoch 5/10\n",
      "160/180 [=========================>....] - ETA: 1s - loss: 0.5052 - accuracy: 0.9000\n",
      "Epoch 00005: val_accuracy improved from 0.58333 to 0.71111, saving model to best_model_trial_3.h5\n",
      "180/180 [==============================] - 23s 129ms/sample - loss: 0.5146 - accuracy: 0.9000 - val_loss: 0.5667 - val_accuracy: 0.7111\n",
      "Epoch 6/10\n",
      "160/180 [=========================>....] - ETA: 1s - loss: 0.2158 - accuracy: 0.9375\n",
      "Epoch 00006: val_accuracy improved from 0.71111 to 0.71667, saving model to best_model_trial_3.h5\n",
      "180/180 [==============================] - 23s 128ms/sample - loss: 0.2076 - accuracy: 0.9333 - val_loss: 0.4879 - val_accuracy: 0.7167\n",
      "Epoch 7/10\n",
      "160/180 [=========================>....] - ETA: 1s - loss: 0.2853 - accuracy: 0.9187\n",
      "Epoch 00007: val_accuracy improved from 0.71667 to 0.76111, saving model to best_model_trial_3.h5\n",
      "180/180 [==============================] - 23s 130ms/sample - loss: 0.2633 - accuracy: 0.9222 - val_loss: 0.4358 - val_accuracy: 0.7611\n",
      "Epoch 8/10\n",
      "160/180 [=========================>....] - ETA: 1s - loss: 0.2624 - accuracy: 0.9438\n",
      "Epoch 00008: val_accuracy improved from 0.76111 to 0.83333, saving model to best_model_trial_3.h5\n",
      "180/180 [==============================] - 24s 131ms/sample - loss: 0.2682 - accuracy: 0.9444 - val_loss: 0.3361 - val_accuracy: 0.8333\n",
      "Epoch 9/10\n",
      "160/180 [=========================>....] - ETA: 1s - loss: 0.0703 - accuracy: 0.9750\n",
      "Epoch 00009: val_accuracy did not improve from 0.83333\n",
      "180/180 [==============================] - 22s 119ms/sample - loss: 0.1146 - accuracy: 0.9556 - val_loss: 0.4776 - val_accuracy: 0.8111\n",
      "Epoch 10/10\n",
      "160/180 [=========================>....] - ETA: 2s - loss: 0.0811 - accuracy: 0.9812\n",
      "Epoch 00010: val_accuracy did not improve from 0.83333\n",
      "180/180 [==============================] - 22s 122ms/sample - loss: 0.0722 - accuracy: 0.9833 - val_loss: 1.8045 - val_accuracy: 0.5333\n"
     ]
    }
   ],
   "source": [
    "#Fit Model!\n",
    "#Replace text_x, test_y with validation data collected!\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10)\n",
    "mc = ModelCheckpoint(model_name, monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)\n",
    "history = cnn_model.fit(train_x, train_y, epochs = 10, batch_size = 32, validation_split = 0.1, verbose = 1, callbacks = [es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Data\n",
    "loss, acc = cnn_model.evaluate(test_x, test_y, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.8044600711928473"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.53333336"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Way solver!\n",
    "#Trail 1: 75% - 52 %\n",
    "#Trail 2: 88.89% - 57%\n",
    "#Trail 3\" 83%  - 53%"
   ]
  }
 ]
}